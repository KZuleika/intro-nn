{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "Numpy es el paquete para computación científica de Python, su principal ventaja es que permite realizar cómputos numéricos con datos vectoriales o matriciales en lugar de tener que implementar bucles lo que resulta en cálculos más rápidos y eficaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ejemplo de función exponencial\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # el resultado es (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si X es un vector, entonces una operación como $s = x + 3$ o $s = \\frac{1}{x}$ producirá una salida s en formato de vector del mismo tamaño que x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de operación con vectores\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementa la función sigmoide usando numpy. \n",
    "\n",
    "**Instrucciones**: x puede ser ahora un número real, un vector o una matriz. La estructura de datos usada en numpy para representar estos datos se llaman numpy arrays.\n",
    "$$ \\text{Para } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calcular la función sigmoide de x\n",
    "\n",
    "    Argumentos:\n",
    "    x -- Un escalar o arreglo numérico de cualquier dimensión\n",
    "\n",
    "    Devuelve:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Escribir aquí el código ###\n",
    "    s = 1/(np.exp(-x) + 1)\n",
    "    ### Aquí termina el código ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de la función sigmoide\n",
    "\n",
    "**Ejercicio**: Implementa la función sigmoid_grad() para calcular el gradiente de la función sigmoide con respecto a la entrada x. La fórmula es: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "Se puede implementar esta función en dos pasos:\n",
    "1. Definir s como la función sigmoide de x, para lo cual se puede usar la función definida en el ejercicio anterior.\n",
    "2. Calcular $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de la función sigmoide con respecto a su entrada x.\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- Un escalar o arreglo numérico de cualquier dimensión\n",
    "\n",
    "    Devuele:\n",
    "    ds -- El gradiente calculado.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Escribir aquí el código ###\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    ### Aquí termina el código ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiar la forma de los arreglos ###\n",
    "\n",
    "Dos funciones usadas comunmente en aprendizaje profundo son [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) y [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape se usa para obtener la forma (dimensiones y tamaño) de un arreglo X. \n",
    "- X.reshape(...) se usa para cambiar la forma (dimensiones y tamaño) de X.\n",
    "\n",
    "Por ejemplo, una imágen se representa normalmente por un arreglo de 3 dimensiones $(ancho, alto, profundidad = 3)$. Sin embargo, cuando se lee una imágen como la entrada de un algoritmo, se convierte en un vector de la forma $(ancho*alto*3, 1)$. En otras palabras, se \"desenrrolla\" o cambia la forma del arreglo de 3 dimensiones en un vector (1 dimensión).\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Ejercicio**: Implementa `image2vector()` que toma una entrada de la forma (ancho, alto, 3) y regresa un vector de la forma (ancho\\*alto\\*3, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    image -- un arreglo de numpy de la forma (ancho, alto, profundo)\n",
    "    \n",
    "    Regresa:\n",
    "    v -- un vector de la forma (ancho*alto*profundo, 1)\n",
    "    \"\"\"\n",
    "    ### Escribir aquí el código ###\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    ### Aquí termina el código ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de un arreglo de la forma (3, 3, 2)\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar las filas\n",
    "\n",
    "Otra técnica común usada en Machine Learning es normalizar los datos. Frecuentemente ayuda a un mejor desempeño ya que la técnica de descenso por gradiente converge más rápido después de la normalización. Normalizar significa cambiar x a $ \\frac{x}{\\| x\\|} $ (dividiendo cada fila de la matriz por su norma).\n",
    "\n",
    "Por ejemplo,si $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ entonces $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$y        $$ x\\_normalizada = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ Se puede notar que es posible dividir matrices de diferentes tamaños, esto se conoce en numpy como \"broadcasting\".\n",
    "\n",
    "\n",
    "**Ejercicio**: Implementar normalizeRows() para normalizar las filas de una matriz. Después de aplicar esta función a una matriz x, cada fila de x debe ser un vector de longitud unitaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implementar una función que normaliza cada fila de la matriz x.\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- Una matriz de numpy de la forma (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    ### Escribir aquí el código ###\n",
    "    x_norm = np.linalg.norm(x,axis = 1,keepdims=True)\n",
    "    x = x/x_norm\n",
    "    ### Aquí termina el código ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> **normalizeRows(x)** </td> \n",
    "       <td> [[ 0.          0.6         0.8       ]\n",
    " [ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting y la función softmax ####\n",
    "Un concepto importante es el de \"Bradcasting\" ya que es muy útil para realizar operaciones matemáticas entre arreglos de diferentes formas.(http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar una función softmax como función normalizadora cuando el algoritmo necesita clasificar dos o más clases.\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{para } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- para una matriz $x \\in \\mathbb{R}^{m \\times n}$ donde, $x_{ij}$ se refiere al elemento en la fila $i^{th}$ y la columna $j^{th}$ de $x$, entonces tenemos:  \n",
    "$$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(primera fila de x)}  \\\\\n",
    "    softmax\\text{(segunda fila de x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(última fila de x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calcula la función softmaz para cada fila de la entrada x.\n",
    "\n",
    "    El código debe funcionar para un vector fila o también para matrices de la forma (m,n).\n",
    "\n",
    "    Argumentos:\n",
    "    x -- Una matriz Numpy de la forma (m,n)\n",
    "\n",
    "    Retorna:\n",
    "    s -- Una matriz de numpy igual a la función softmax de x, de la forma (m,n).\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Escribir aquí el código ###\n",
    "    # Aplicar exp() a cada elemento de x. Usar np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    # Crear un vector x_sum que sume cada fila de x_exp. Usar np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n",
    "    \n",
    "    # Calcular softmax(x) by dividiendo x_exp por x_sum.\n",
    "    s = x_exp/x_sum\n",
    "   ### Aquí termina el código ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida Esperada**:\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> **softmax(x)** </td> \n",
    "       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
    "    1.21052389e-04]\n",
    " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
    "    8.01252314e-04]]</td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning se trabaja con conjuntos de datos muy grandes, por lo que una función que no es óptima computacionalmente puede volverse un gran cuello de botella en el algoritmo y puede resultar en un modelo que tarda demasiado en correr. Para asegurarse de que el código es computacionalmente eficiente, es necesario usar la vectorización a lo largo de todos los ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar las funciones de costo L1 y L2.\n",
    "\n",
    "**Ejercicio**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Recordatorio**:\n",
    "- el costo L1 está definido como:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    yhat -- vector de tamaño m (predicción de etiquetas)\n",
    "    y -- vector de tamaño m (etiquetas verdaderas)\n",
    "    \n",
    "    Regresa:\n",
    "    costo -- El cotso L1\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Escribir aquí el código ###\n",
    "    loss = sum(abs(yhat - y))\n",
    "    ### Aquí termina el código ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L1** </td> \n",
    "       <td> 1.1 </td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**: Implementar la versión vectorizada de el costo L2. Puede resultar útil la función np.dot(). Como recordatorio, si $x = [x_1, x_2, ..., x_n]$, entonces `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- el costo L2 está definido como $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Escribir aquí el código ###\n",
    "    x = yhat - y\n",
    "    loss = np.dot(x,x)\n",
    "    ### Aquí termina el código ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
